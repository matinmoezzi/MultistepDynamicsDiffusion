defaults:
  - _self_
  - env: mbpo_humanoid
  # - hydra/sweeper: svg
  # - hydra/launcher: submitit_slurm

format_strs:
  - log
  - stdout
  - csv
  - tensorboard

sweep:
  n_sample: 50
  n_seed: 10

replay_buffer_capacity: min(${num_train_steps}, 1e6)

num_seed_steps: 1000

# For model pre-training
model_pretrain_seed_steps: 500000
model_pretrain_update_repeat: 5

eval_freq: ??? # Over-ridden by env
num_eval_episodes: 5
fixed_eval: false

pixels: false
normalize_obs: true

action_repeat: 1

log_freq: 500

save_video: false
delete_replay_at_end: false

save_freq: 100000
save_best_eval: true
save_zfill: 7

device: cuda
checkpoint_path: ""

seed: 1

# For debugging:
num_initial_states: null # Use a subset of initial states
max_episode_steps: null # If set, use shorter episodes

model_free_hidden_dim: 512
model_free_hidden_depth: 4
obs_dim: ???
action_dim: ???
action_range: ???


agent:
  _target_: multistep_dynamicsdiffusion.agent.DynamicsDiffusionAgent
  env_name: ${env_name}
  obs_dim: ${obs_dim}
  action_dim: ${action_dim}
  action_range: ${action_range}
  dx: ${dx}
  num_train_steps: ${num_train_steps}


  temp: ${learn_temp}
  # temp_cfg: null # auto-set to best for env+agent if null

  actor: ${normal_actor}
  actor_lr: 1e-4
  actor_betas: [0.9, 0.999]
  actor_update_freq: 1
  actor_mve: true
  actor_detach_rho: false
  actor_dx_threshold: null

  critic: ${double_q_critic}
  critic_lr: 1e-4
  critic_tau: 0.005
  critic_target_update_freq: 1
  critic_target_mve: false
  full_target_mve: False

  discount: 0.99
  seq_batch_size: 128
  step_batch_size: 64
  horizon: 3
  seq_train_length: ${agent.horizon}
  update_freq: 1

  model_update_freq: 1
  model_update_repeat: 1

  model_free_update_repeat: 1

  rew_hidden_dim: 512
  rew_hidden_depth: 2
  rew_lr: 1e-3

  done_hidden_dim: 512
  done_hidden_depth: 2
  done_lr: 1e-3
  done_ctrl_accum: true

  warmup_steps: 0 # Auto-set if null

  det_suffix: 0.0

dx:
  _target_: multistep_dynamicsdiffusion.diffusion_dx.DiffusionDx
  scheduler:
    _target_: diffusers.DPMSolverMultistepScheduler
  env_name: ${..env_name}
  obs_dim: ${..obs_dim}
  action_dim: ${..action_dim}
  detach_xt: true
  clip_grad_norm: null
  lr: 1e-4
  num_inference_steps: 20
  ema_rate: 0.9999
  model:
    _target_: multistep_dynamicsdiffusion.transformer.TransformerForDiffusion
    input_dim: ${..obs_dim}
    output_dim: ${..obs_dim}
    horizon: ${...agent.horizon}
    act_dim: ${..action_dim}

    n_layer: 2
    n_head: 2
    n_emb: 64
    p_drop_emb: 0.0
    p_drop_attn: 0.1

    causal_attn: False
    n_cond_layers: 2 # >0: use transformer encoder for cond, otherwise use MLP

normal_actor:
  _target_: multistep_dynamicsdiffusion.actor.TransformerActor
  obs_dim: ${obs_dim}
  action_dim: ${action_dim}
  hidden_dim: 64 
  num_layers: 1
  num_heads: 2
  horizon: ${agent.horizon}
  log_std_bounds: [-5,2]

double_q_critic:
  _target_: multistep_dynamicsdiffusion.critic.DoubleQCritic
  obs_dim: ${obs_dim}
  action_dim: ${action_dim}
  n_emb: 64 
  num_layers: 1 
  num_heads: 4

learn_temp:
  _target_: multistep_dynamicsdiffusion.temp.LearnTemp
  init_temp: 0.1
  max_steps: ${num_train_steps}
  init_targ_entr: -${action_dim}
  final_targ_entr: -${action_dim}
  entr_decay_factor: 0.
  only_decrease_alpha: false
  lr: 1e-4

hydra:
  searchpath:
    - file://config
  run:
    dir: ./logs/${env_name}/${device}${oc.decode:${oc.env:WORLD_SIZE,1}}_${human_readable_steps:${num_train_steps}}_H${agent.horizon}_${now:%Y%m%d-%H:%M:%S}
  sweep:
    dir: ./exp/${now:%Y.%m.%d}/${now:%H%M}_${experiment}
    subdir: ${hydra.job.num}
  # launcher:
  #   max_num_timeout: 100000
  #   timeout_min: 4319
  #   partition: scavenge
  #   mem_gb: 64
  #   gpus_per_node: 1
